This tutorial is for web log analytic. It extracts session data from logs

Generate product data
=====================
./gen_product.rb <num_of_products> <prod_id_length> > product.txt

num_of_products = number of products e.g. 100
prod_id_length = lengh of priduct id field e.g., 10

Generate user data
==================
./gen_user.rb <num_of_users> <user_id_length> > user.txt

num_of_users = number of users e.g. 1000
user_id_length = lengh of user id field e.g., 10

Generate converted users
========================
Take some lines from user.txt and copy into a new file convertedUsers.txt

Generate logs
=============
./gen_log.rb <date> <num_of_users> > log.txt
date = date e.g. 2015-05-15
num_of_users = number of users e.g. 200

Search for remFromCart in the logfile. If it does not exist at least once,
regenerate log data until that token is found.

Copy log data to HDFS input path
================================
hadoops fs -put log.txt /user/pranab/sed/input

Run session extractor map reduce
================================
JAR_NAME=/home/pranab/Projects/visitante/target/visitante-1.0.jar
CLASS_NAME=org.visitante.basic.SessionEventDetector
echo "running mr"
IN_PATH=/user/pranab/sed/input
OUT_PATH=/user/pranab/sed/output
echo "input $IN_PATH output $OUT_PATH"
hadoop fs -rmr $OUT_PATH
echo "removed output dir"
hadoop jar $JAR_NAME  $CLASS_NAME -Dconf.path=/home/pranab/Projects/bin/visitante/visitante.properties  $IN_PATH  $OUT_PATH

Configuration
=============
Configutation properiies are in visitante.properties

